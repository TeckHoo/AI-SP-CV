{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e854db97-3ebb-4090-9672-37b3d7185bdf",
   "metadata": {},
   "source": [
    "# **Practical Assignment 2: Real-time Object Detection System Development and Solution Study**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b3613-8b78-45cb-a1fc-e44b9390cd4b",
   "metadata": {},
   "source": [
    "In this Practical Assignment, you will be performing fine-tuning of an object detection model, which may consume too much resources for Google Colab (free version) to handle. Thus we will try to run the code on our personal laptops, which also makes it easier to connect to our laptop webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35fc86-35af-4e18-8efe-9e9672f01742",
   "metadata": {},
   "source": [
    "**Note**: The base environment provided by Anaconda may not have the correct library versions installed. It's best to start from a new \"environment\", and you should be opening up this Notebook from inside the new environment we will create together in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c282c-8046-4894-b84a-3c590150849a",
   "metadata": {},
   "source": [
    "# 1. Requirement\n",
    "This is a capstone assignment, in which you will learn practical object detection technology and solution. The assignment will guide you to develop a real-time object detection system with updated YOLO framework, and you will engage in end-to-end practice, encompassing the entire pipeline from image labelling, model finetuning to webcam deployment. In details, the assignment is comprised of three parts:\n",
    "* Finetune a basic YOLO model for wall crack detection. \n",
    "* Retrain the model for live video detection, object tracking and counting.  \n",
    "* Explore the solutions for diverse object detection applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711cf26-9798-40a4-8653-9277bd7be901",
   "metadata": {},
   "source": [
    "Referring to '**practical_assignment_2.docx**' for the detailed instructions, write down experiment steps, results, answers and literature study into '**practical_assignment_2.docx**', and expand it into your final assignment report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c05102-e8ae-4feb-a5fb-86d666cc4d5c",
   "metadata": {},
   "source": [
    "# 2. Fine-tune YOLO11 model for crack detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95792d24-56fd-46a8-b0eb-d007ae9dafba",
   "metadata": {},
   "source": [
    "## 1. Set up development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa68c28-b59a-41f0-871b-10b6f6c1d33d",
   "metadata": {},
   "source": [
    "### Install ultralytics library\n",
    "\n",
    "And run some system analysis :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1f5cc-768b-4f05-a0fe-f44c6cc1d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first time you run the new environment, need to install ultralytics packages and dependencies\n",
    "%pip install numpy matplotlib\n",
    "%pip install ultralytics\n",
    "\n",
    "# Load the libraries to make sure everything's fine\n",
    "import matplotlib.pyplot as plt\n",
    "import ultralytics\n",
    "\n",
    "# So that our matplotlib plots show up in the notebook itself\n",
    "%matplotlib inline\n",
    "\n",
    "# Check software and hardware\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48af35e-14dd-497e-90b6-0af574a0c607",
   "metadata": {},
   "source": [
    "### Run inference on a test image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e4daa-0698-42d0-9084-63054ef4ad42",
   "metadata": {},
   "source": [
    "The detection results will open in a new window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2167f-68fc-4c12-ae3c-d60ce499af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11n.pt\")  # initialize model\n",
    "results = model([\"https://ultralytics.com/images/bus.jpg\"])  # perform inference\n",
    "results[0].show()  # display results for the first image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcba496-a021-494e-bf02-72152743c0cd",
   "metadata": {},
   "source": [
    "### Set working directories / file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfe5e7-ce8a-4cb9-8169-7d9a363c1831",
   "metadata": {},
   "source": [
    "Please change the following file paths to match where you saved the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd5ddbb-93bb-48d7-9f66-aadfb5519b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_PA2 = r'C:\\Users\\xiaoming\\PA2'                       # (Windows) This should be the directory where your PA2 files are all stored\n",
    "                                                                       # On Mac/Linux, file paths will look like '/Users/S11009880/...' or '/home/S1100...'\n",
    "path_yaml = os.path.join(path_PA2, r'datasets\\crack400.yaml')          # Config file for YOLO model training\n",
    "saved_model = os.path.join(path_PA2, 'best.pt')                        # Model weights for our best-performing trained model\n",
    "path_test = os.path.join(path_PA2, r'datasets\\crack400\\images\\test') # Test dataset images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917cd2f-41f0-4279-ada3-6ea50049e964",
   "metadata": {},
   "source": [
    "## 2. Prepare cracks dataset\n",
    "\n",
    "- Download and unzip `datasets.zip`, which contains images of wall/pavement cracks\n",
    "  - Currently, all images are in the `raw` folder\n",
    "  - The directory structure for model training and evaluation has been set up for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3aba6-e2b7-405d-99f3-01eb27d95a5c",
   "metadata": {},
   "source": [
    "## 3. Label / annotate images\n",
    "\n",
    "We will annotate our images using the LabelImg program\n",
    "- Setup:\n",
    "  - Download from Teams and unzip `labelImg.zip`\n",
    "  - Prepare for dataset annotation by changing predefined_classes.txt to only have one class: crack\n",
    "\n",
    "- Annotate the `train`, `val`, and `test` images\n",
    "\n",
    "- Move images (`.jpg`) and annotations (`'.txt`) into respective folders\n",
    "\n",
    "- Move `test` and `val` background images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33b710-9373-45a3-a43c-21a74f932c9d",
   "metadata": {},
   "source": [
    "## 4. Create dataset config file\n",
    "\n",
    "* Change the `path` to where your `crack400` folder is\n",
    "  * You should not need to change the paths for the train/val/test directories\n",
    "  * Ensure that the class IDs and names are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b3e40-9643-45e2-afb3-b38812660169",
   "metadata": {},
   "source": [
    "## 5. Train and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00531ad-70d5-4de3-ad5b-074db5644acb",
   "metadata": {},
   "source": [
    "Here, we will load a pre-trained model and train it on our cracks dataset.\n",
    "\n",
    "You are recommended to use `'yolo11n.pt'`. A suitable (older) alternative model is `'yolov8n.pt'`.\n",
    "\n",
    "When running the following cell, the last line will start the training process, and displays the following outputs:\n",
    "1. The full set of training parameters used (inlcuding the defaults)\n",
    "2. The architecture of the YOLO model (much like the PyTorch model summaries we saw in CNNs for classification)\n",
    "3. Some status information for the loading of train and val datasets\n",
    "4. Progress bars that will be updated as model training goes on for each epoch\n",
    "   - After each epoch, the model is evaluated against the Val dataset, giving the **P**recision, **R**ecall, **mAP50**, and **mAP50-95**.\n",
    "5. Evaluation reaults of the best model so far\n",
    "\n",
    "**Note**: The last lines in the output will indicate where the trained weights are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab142807-b079-48bd-8bf7-b4b43b278f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLO model\n",
    "model1 = YOLO('yolo11n.pt')\n",
    "\n",
    "# Train the model on crack dataset for 5 epochs\n",
    "num_epochs = 5  # Number of epochs\n",
    "lr0 = 0.01      # What do lr0 and lrf represent? See documentation at https://docs.ultralytics.com/modes/train/#train-settings\n",
    "lrf = 0.01\n",
    "batch = 8       # Batch size\n",
    "\n",
    "results = model1.train(data=path_yaml, epochs=num_epochs, lr0=0.01, lrf=0.01, batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c51ae-9c56-4ed6-ab5c-10d433d08e98",
   "metadata": {},
   "source": [
    "After training for a few epochs, we will load the best model obtained and evaluate it against the **Test** dataset. \n",
    "\n",
    "**Note**:\n",
    "- The last line in the output above should read: `Results saved to ...\\runs\\detect\\trainX`, where `X` is some number\n",
    "  - Set `traindir` to this directory to this directory!\n",
    "  - Each run will create a different folder for weights to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca3876-a0d6-4dbb-823c-bc3475b7a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best YOLO model\n",
    "traindir = r'C:\\Users\\xiaoming\\PA2\\runs\\detect\\train2'\n",
    "weightsfile = os.path.join(traindir, r'weights\\best.pt')\n",
    "model2 = YOLO(weightsfile)\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "results = model2.val(data=path_yaml, split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8ffaf-d91f-4e45-affe-d5b9d90da2ac",
   "metadata": {},
   "source": [
    "### **[Task 1]** Study the dataset and output information, answer the questions below, and write into your report. <br>\n",
    "\n",
    " * How many images have you labelled for training, validation and test respectively?\n",
    " * What is original image size (H x W)?\n",
    " * What is input image size (H x W) required by the chosen YOLO network?\n",
    " * What is your model (best.pt) file size?\n",
    " * What is your model optimizer and learning rate (lr)?\n",
    " * How many layers and parameters does your model have?\n",
    " * What is number of classes (attribute `nc`) in pretrained YOLO model (`model1`)? And what is number of classes (`nc`) in your best.pt model (`model2`)?\n",
    " * What is your model (best.pt) mAP50 and mAP50-95 on validation set?\n",
    " * What is your model (best.pt) mAP50 and mAP50-95 on test set?\n",
    " * Among accuracy, recall and precision, which metric is more important for the building quality assurance? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec0eb6-5c51-4ea6-b38d-cc4e1b67be6b",
   "metadata": {},
   "source": [
    "## 6. Finetune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ce0c4-9bc5-4e02-99f7-45f022c5f7a2",
   "metadata": {},
   "source": [
    "Reload the pre-trained model, and try training + evaluating the trained model using different training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63319b34-7c5a-4b96-90e3-5c045478165a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Following the Task 2 below, retrain and evaluate the model\n",
    "# Referring to https://docs.ultralytics.com/modes/train/#usage-examples, adjust training arguments (hyperparameters)\n",
    "\n",
    "# Load a pretrained YOLO model\n",
    "model1 = YOLO('yolo11n.pt')\n",
    "# Retrain the model \n",
    "num_epochs = 10\n",
    "lr0 = 0.01     \n",
    "lrf = 0.01\n",
    "batch = 8      \n",
    "\n",
    "results = model1.train(data=path_yaml, epochs=10, lr0=0.01, lrf=0.01, batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f42898-797b-44eb-9ed6-e68796944772",
   "metadata": {},
   "source": [
    "Reload the best model weights, and evaluate on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c26238-6c6a-4568-b0d6-e6f8c43501e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate new model's performance on the test set\n",
    "traindir = r'C:\\Users\\xiaoming\\PA2\\runs\\detect\\train10'\n",
    "weightsfile = os.path.join(traindir, r'weights\\best.pt')\n",
    "model2 = YOLO(weightsfile)\n",
    "results = model2.val(data=path_yaml, split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1eaa4-a8ff-4813-a10c-c9da2fefce34",
   "metadata": {},
   "source": [
    "Plot and save the detections annotated on test images. The output will indicate where to find the annotated test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461a615-2a5a-4e19-a99f-80961d2c77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with test images:\n",
    "results = model2.predict(path_test, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7a1be-1c30-46b5-a484-e3c9558f56b5",
   "metadata": {},
   "source": [
    "### **[Task 2]** Write down experiment steps, settings, results and conclusions to the report. <br>\n",
    " * Adjust epochs with different values like 10, 20, 30, 50 and 100, retrain the model, study how epoch value impacts model performance (mAP50 and mAP50-95) on test set.\n",
    " * Adjust learning rate (lr0 and lrf) with different values like 0.01, 0.02, 0.03, 0.001, 0.002, 0.003, retrain the model, study how learning rate impacts model performance (mAP50 and mAP50-95) on test set.\n",
    " * Adjust batch with different size like 8, 16, 32, retrain the model, study how batch size impacts model performance (mAP50 and mAP50-95) on test set.\n",
    " * Check ‘confusion_matrix.png’ in ‘runs/detect/val/’, write down metrics (TP, FP, TN, FN) in confusion matrix.\n",
    " * Copy all the images from ‘train_background’ folder to ‘train’ folder.\n",
    "   * If you had already included the train_background pictures in training, try deleting these backgroud images (the file names all start with `bg`) and then rerun the training and evaluation.\n",
    " * Retrain the model and validate the model with test set.\n",
    " * Check ‘confusion_matrix.png’ in ‘runs/detect/val/’, study how the metrics (TP, FP, TN, FN) in confusion matrix are impacted after adding background images to training set.\n",
    " * Test the model through ‘model2.predict()’ command given above, find prediction results in ‘runs/detect/prediction#’ folder, and check whether the detection is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476cc3bc-91f8-4102-87fd-8b4dc148eb06",
   "metadata": {},
   "source": [
    "**Note from Zhen Wah**: Part 2 will be in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b8edb-8789-4faf-993c-1665abeb4994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
